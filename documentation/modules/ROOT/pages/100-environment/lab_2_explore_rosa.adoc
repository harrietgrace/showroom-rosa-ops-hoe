include::../_attributes.adoc[]

== VARS test

[source,subs="attributes"]
----
rosa_admin_user: {rosa_admin_user}
rosa_admin_password: {rosa_admin_password}
rosa_api_url: {rosa_api_url}

rosa_bastion_user_name: {rosa_bastion_user_name}
rosa_console_url: {rosa_console_url}
rosa_console_user_name: {rosa_console_user_name}
rosa_console_password: {rosa_console_password}
rosa_sandbox_account_id: {rosa_sandbox_account_id}

rosa_subdomain_base: rosa_subdomain_base
rosa_token_warning: {rosa_token_warning}

ssh_command: {ssh_command}
ssh_username: {ssh_username}
ssh_password: {ssh_password}
rosa_user_password: {rosa_user_password}

targethost:{targethost}
----

== Explore your ROSA environment

As you have seen in the video your ROSA cluster has already been deployed for you. This is both to speed things up for this lab and because your AWS user does not have sufficient permissions to deploy a new ROSA cluster - only enough to follow these lab instructions.

All interactions with ROSA happen through the `rosa` CLI (or on the Red Hat Console web page - to which you also do not have access). So let's use the CLI to explore what you have.

. List all deployed ROSA clusters in the AWS account:
+
[source,sh,role=execute]
----
rosa list clusters
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
ID                                NAME         STATE  TOPOLOGY
26mpkbt35pras0n6867p732qeb1l7tmm  rosa-wkrh33  ready  Hosted CP
----
+
[TIP]
====
The ID and Name of your cluster will be different than in the sample output above.

Also notice the Topology and state. State should be *ready* and Topology could be either *Classic (STS)* or *Hosted CP* which stands for Hosted Control Plane - a new, resource saving way of deploying a ROSA cluster.
====

. Now let's examine this cluster a bit more by describing the cluster (the `$GUID` environment variable is already set for you so you can immediately describe your individual cluster):
+
[source,sh]
----
rosa describe cluster --cluster rosa-$GUID
----
+
.Sample Output
[source,texinfo,subs="attributes"]
----
Name:                       rosa-wkrh33
ID:                         26mpkbt35pras0n6867p732qeb1l7tmm
External ID:                45f44186-94a7-4444-a6ce-2f82c1afe09a
Control Plane:              ROSA Service Hosted
OpenShift Version:          4.12.35
Channel Group:              stable
DNS:                        rosa-wkrh33.8jh1.p3.openshiftapps.com
AWS Account:                551773360602
API URL:                    {rosa_api_url}
Console URL:                {rosa_console_url}
Region:                     eu-central-1
Availability:
 - Control Plane:           MultiAZ
 - Data Plane:              SingleAZ
Nodes:
 - Compute (desired):       2
 - Compute (current):       2
Network:
 - Type:                    OVNKubernetes
 - Service CIDR:            172.30.0.0/16
 - Machine CIDR:            10.0.0.0/16
 - Pod CIDR:                10.128.0.0/14
 - Host Prefix:             /23
Workload Monitoring:        Enabled
Ec2 Metadata Http Tokens:   optional
STS Role ARN:               arn:aws:iam::551773360602:role/ManagedOpenShift-HCP-ROSA-Installer-Role
Support Role ARN:           arn:aws:iam::551773360602:role/ManagedOpenShift-HCP-ROSA-Support-Role
Instance IAM Roles:
 - Worker:                  arn:aws:iam::551773360602:role/ManagedOpenShift-HCP-ROSA-Worker-Role
Operator IAM Roles:
 - arn:aws:iam::551773360602:role/rosa-wkrh33-kube-system-capa-controller-manager
 - arn:aws:iam::551773360602:role/rosa-wkrh33-kube-system-control-plane-operator
 - arn:aws:iam::551773360602:role/rosa-wkrh33-kube-system-kms-provider
 - arn:aws:iam::551773360602:role/rosa-wkrh33-openshift-image-registry-installer-cloud-credentials
 - arn:aws:iam::551773360602:role/rosa-wkrh33-openshift-ingress-operator-cloud-credentials
 - arn:aws:iam::551773360602:role/rosa-wkrh33-openshift-cluster-csi-drivers-ebs-cloud-credentials
 - arn:aws:iam::551773360602:role/rosa-wkrh33-openshift-cloud-network-config-controller-cloud-cred
 - arn:aws:iam::551773360602:role/rosa-wkrh33-kube-system-kube-controller-manager
Managed Policies:           Yes
State:                      ready 
Private:                    No
Created:                    Oct  6 2023 10:57:55 UTC
Details Page:               https://console.redhat.com/openshift/details/s/2WO3c1TcyGrdKPznnS51HYdL6dn
OIDC Endpoint URL:          https://rh-oidc.s3.us-east-1.amazonaws.com/26mpk4vkj1n1phqpb6ai7hf5c5oavold (Managed)
Audit Log Forwarding:       disabled
----
+
Note that not all values in the sample output will match exactly your environment.

. Get the API URL for your cluster:
+
[source,sh]
----
rosa describe cluster --cluster rosa-$GUID --output json | jq .api.url
----
+
.Sample Output
[source,texinfo,subs="attributes"]
----
{rosa_api_url}
----

. Get the OpenShift Console URL for your cluster:
+
[source,sh]
----
rosa describe cluster --cluster rosa-$GUID --output json | jq .console.url
----
+
.Sample Output
[source,texinfo,subs="attributes"]
----
{rosa_console_url}
----

. A temporary admin user has already been created for you on the ROSA OpenShift cluster.
+
[source,texinfo,subs="attributes"]
====
Admin user ID: {rosa_admin_user}
Admin user Password: {rosa_admin_password}
====

. Now that you have the information about the Admin credentials and the API URL for your cluster you can log into your cluster:
cmdo
+
[source,sh,subs="attributes"]
----
oc login --username {rosa_admin_user} --password {rosa_admin_password} {rosa_api_url}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y

WARNING: Using insecure TLS client config. Setting this option is not supported!

Login successful.

You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
Welcome! See 'oc help' to get started.
----
+
[TIP]
====
If you get an error that the *Login failed (401 Unauthorized)* wait a few seconds and then try again. It takes a few minutes for the cluster authentication operator to update itself after creating the cluster admin user.
====

. To check that you are logged in as the admin user you can run `oc whoami`:
+
[source,sh,role=execute]
----
oc whoami
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
cluster-admin
----

. You can also confirm by running the following command. Only a cluster-admin user can run this without errors.
// +
// Also these pods may not yet exist or be in `Pending` status depending on how long it's been since your control plane finished installing.
+
[source,sh,role=execute]
----
oc get pod -n openshift-ingress
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                              READY   STATUS    RESTARTS   AGE
router-default-7994f6fd58-8cl45   1/1     Running   0          102s
router-default-7994f6fd58-z6gpm   1/1     Running   0          102s
----

. You can now use the cluster as an admin user, which will suffice for this workshop. Though, for any other use, it is highly recommended to set up an IdP.

// === Wait for Cluster Operators to finish rolling out

// Once your cluster has worker nodes available the cluster operators can deploy their operands to these worker nodes. These include the console and ingress controllers for example. Without those it will be impossible to access the cluster.

// . Repeat the following command every few minutes until the output looks like the example below (all operators should show as available, not progressing and certainly not degraded):
// +
// [source,sh,role=execute]
// ----
// oc get co
// ----
// +
// .Sample Output
// [source,texinfo,options=nowrap]
// ----
// NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
// console                                    4.12.18   True        False         False      2m41s
// csi-snapshot-controller                    4.12.18   True        False         False      16m
// dns                                        4.12.18   True        False         False      5m30s
// image-registry                             4.12.18   True        False         False      4m51s
// ingress                                    4.12.18   True        False         False      4m56s
// insights                                   4.12.18   True        False         False      6m8s
// kube-apiserver                             4.12.18   True        False         False      16m
// kube-controller-manager                    4.12.18   True        False         False      16m
// kube-scheduler                             4.12.18   True        False         False      16m
// kube-storage-version-migrator              4.12.18   True        False         False      6m3s
// monitoring                                 4.12.18   True        False         False      3m40s
// network                                    4.12.18   True        False         False      16m
// node-tuning                                4.12.18   True        False         False      6m37s
// openshift-apiserver                        4.12.18   True        False         False      16m
// openshift-controller-manager               4.12.18   True        False         False      16m
// openshift-samples                          4.12.18   True        False         False      5m6s
// operator-lifecycle-manager                 4.12.18   True        False         False      16m
// operator-lifecycle-manager-catalog         4.12.18   True        False         False      16m
// operator-lifecycle-manager-packageserver   4.12.18   True        False         False      16m
// service-ca                                 4.12.18   True        False         False      6m5s
// storage                                    4.12.18   True        False         False      6m27s
// ----

=== Login to the OpenShift Web Console

Next, let's log in to the OpenShift Web Console. To do so, follow the below steps:

. First, we'll need to grab your cluster's web console URL. To do so, run the following command:
+
[source,sh,role=execute]
----
oc whoami --show-console
----
+
.Sample Output
[source,text,options=nowrap]
----
https://console-openshift-console.%rosa_subdomain_base%
----

. Next, open the printed URL in a web browser.
//. Click on the `htpasswd` identity provider.
. Enter the username (`cluster-admin`) and password from the previous section (use `echo ${ADMIN_PASSWORD}` to remind yourself what the password is in case you forgot).
+
If you don't see an error, congratulations! You're now logged into the cluster and ready to move on to the workshop content.
